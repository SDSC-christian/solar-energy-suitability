{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from collections import defaultdict\n",
    "from PIL import Image, ImageDraw\n",
    "from typing import Optional\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling the data\n",
    "We used [encord](https://encord.com) to annotate our dataset, marking regions within the tiles using **polygons** assigned one of two labels: `solar_panel` or `unsure`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import and cleaning\n",
    "The labeled data was exported in a `.json` format file. Only a subset of a the fields in the file are necessary for our model namely:\n",
    "- `dataset_title`: The seed used to generate the tile\n",
    "- `data_title`: The name of the tile\n",
    "- `polygon`: The vertices of the polygons covering the solar panels\n",
    "- `value`: the label of the polygon (either `solar_panel` or `unsure`)\n",
    "\n",
    "We will convert the labeled data into a `pandas` data frame and while only keeping these fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(seed: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the labels for a given seed.\n",
    "\n",
    "    Args:\n",
    "        seed (int): The seed of the dataset.\n",
    "    Returns:\n",
    "        pd.DataFrame: The labels for the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    file_path = f\"../data/labels/LABELS_SEED_{seed}.json\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    extracted_data = [\n",
    "        {\n",
    "            \"seed\": int(entry.get(\"dataset_title\").replace(\"Tiles seed=\", \"\")),\n",
    "            \"data_title\": entry.get(\"data_title\"),\n",
    "            \"polygon\": obj.get(\"polygon\"),\n",
    "            \"value\": obj.get(\"value\"),\n",
    "        } \n",
    "        for entry in data\n",
    "        for unit in entry.get(\"data_units\", {}).values()\n",
    "        for obj in unit.get(\"labels\", {}).get(\"objects\", [{\"polygon\": None, \"value\": None}])\n",
    "    ]\n",
    "    return pd.DataFrame(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_1_df = load_labels(1)\n",
    "seed_1_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the number of tiles that have no polygons, meaning that they do not contain any solar panels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {seed_1_df.isna().sum()[\"polygon\"]} tiles that have no polygons.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create the **mask** for each image indicating the position of the solar pannels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(\n",
    "    df: pd.DataFrame,\n",
    "    image_size: tuple[int, int] = (1000, 1000),\n",
    "    target_value: str = \"solar_panel\",\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Create segmentation masks for images with labeled polygons.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): data frame containing `data_title`, `polygon`, and `value` features.\n",
    "        image_size (tuple): dimensions of the image (width, height).\n",
    "        target_value (str): value to filter polygons for masking (e.g., \"solar_panel\").\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary with `data_title` as keys and 1D numpy arrays (segmentation masks) as values.\n",
    "    \"\"\"\n",
    "    masks = defaultdict(lambda: np.zeros(image_size[0] * image_size[1]))\n",
    "    for _, row in df.iterrows():\n",
    "        if row[\"value\"] == target_value and isinstance(row[\"polygon\"], dict):\n",
    "            mask = Image.new(\"L\", image_size)\n",
    "            draw = ImageDraw.Draw(mask)\n",
    "\n",
    "            polygon_points = [\n",
    "                (vertex[\"x\"] * image_size[0], vertex[\"y\"] * image_size[1])\n",
    "                for vertex in row[\"polygon\"].values()\n",
    "            ]\n",
    "\n",
    "            draw.polygon(polygon_points, outline=1, fill=1)\n",
    "            mask_array = np.array(mask).flatten()\n",
    "            masks[row[\"data_title\"]] += mask_array\n",
    "\n",
    "    masks = {k: np.clip(v, 0, 1) for k, v in masks.items()}\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = create_mask(seed_1_df)\n",
    "image_names = list(labels.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, let's superpose the masks on top of their correponding pictures to make sure the `create_mask` function works as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mask_on_image(\n",
    "    image_path: str,\n",
    "    mask: dict,\n",
    "    image_size: tuple[int, int] = (1000, 1000),\n",
    "    alpha: float = 0.5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot a segmentation mask on top of an image.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the original image file.\n",
    "        mask (np.ndarray): 1D numpy array (flattened) representing the segmentation mask.\n",
    "        image_size (tuple): Dimensions of the image (width, height).\n",
    "        alpha (float): Transparency level of the mask overlay (0 to 1).\n",
    "    \"\"\"\n",
    "    mask_2d = mask.reshape(image_size)\n",
    "    image = Image.open(image_path).resize(image_size)\n",
    "\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    plt.imshow(mask_2d, cmap=\"jet\", alpha=alpha)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Image with Segmentation Mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_image = \"swissimage-dop10_2024_2628.7-1227.8.jpg\"\n",
    "filled_image_path = \"../data/sample/1/\" + filled_image\n",
    "segmentation_mask = labels[filled_image]\n",
    "plot_mask_on_image(filled_image_path, segmentation_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works well! Let's move on to the next step.\n",
    "\n",
    "### Model\n",
    "\n",
    "We will first use a **U-Net** model to segment the solar panels, since in the literature it has been shown to be very effective for this type of task. Let's start with defining our own `Dataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolarPanelDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_dir: str,\n",
    "        image_names: list[str],\n",
    "        masks: list[np.ndarray],\n",
    "        transform: Optional[torchvision.transforms.Compose] = None,\n",
    "    ):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_names = image_names\n",
    "        self.masks = masks\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx) -> Image.Image | np.ndarray:\n",
    "        image_path = os.path.join(self.image_dir, self.image_names[idx])\n",
    "        image = np.array(Image.open(image_path).convert(\"RGB\")).astype(np.float32) / 255.0\n",
    "        mask = self.masks[idx].reshape(1000, 1000).astype(np.float32)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = np.expand_dims(mask, axis=0)\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "dataset = SolarPanelDataset(\"../data/sample/1/\", image_names[:100], list(labels.values())[:100], transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's instantiate a `DataLoader` object to load the data in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [0.8, 0.2])\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define the **U-Net** model. We will use the `torchvision` implementation of the U-Net model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "from unet import UNet\n",
    "\n",
    "# class UNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(UNet, self).__init__()\n",
    "#         self.encoder = nn.Sequential(\n",
    "#             nn.Conv2d(3, 64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "#             nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "#         )\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(64, 1, kernel_size=2, stride=2),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.encoder(x)\n",
    "#         x = self.decoder(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the device on which we will run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's train the model !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(n_channels=3, n_classes=2).to(device)\n",
    "criterion = nn.BCELoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 4\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, masks in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{epochs}\", unit=\"batch\"):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"Validation Loss: {val_loss/len(val_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "\n",
    "sample_image, sample_mask = dataset[4]\n",
    "sample_image = sample_image.unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    predicted_output = model(sample_image)\n",
    "\n",
    "\n",
    "input_image = sample_image.cpu().squeeze().permute(1, 2, 0)\n",
    "if isinstance(sample_mask, np.ndarray):\n",
    "    ground_truth_mask = torch.tensor(sample_mask)\n",
    "\n",
    "ground_truth_mask = ground_truth_mask.cpu().squeeze()\n",
    "predicted_mask = predicted_output.cpu().squeeze()\n",
    "\n",
    "binary_predicted_mask = (predicted_mask > 0.05)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(15, 5))\n",
    "# Input image\n",
    "axes[0].imshow(input_image)\n",
    "axes[0].set_title(\"Input Image\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Ground truth mask\n",
    "axes[1].imshow(ground_truth_mask, cmap=\"gray\")\n",
    "axes[1].set_title(\"Ground Truth Mask\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Predicted mask\n",
    "axes[2].imshow(predicted_mask, cmap=\"gray\")\n",
    "axes[2].set_title(\"Predicted Mask\")\n",
    "axes[2].axis('off')\n",
    "\n",
    "# Thresholded predicted mask\n",
    "axes[3].imshow(binary_predicted_mask, cmap=\"gray\")\n",
    "axes[3].set_title(\"Thresholded Predicted Mask\")\n",
    "axes[3].axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using only roofs to remove background noise\n",
    "\n",
    "Our model is not really good, let's try something else. We can use the output of the `prepare_masks.ipynb` notebook to only keep roofs in images, so that we can remove background noise. Let's load the masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "mask_folder = \"../data/mask/1\"\n",
    "mask_files = [f for f in os.listdir(mask_folder) if f.endswith('.png')]\n",
    "\n",
    "masks = {}\n",
    "for mask_file in mask_files:\n",
    "    mask_path = os.path.join(mask_folder, mask_file)\n",
    "    mask_image = Image.open(mask_path).convert(\"L\")\n",
    "    mask_array = np.array(mask_image) / 255.0\n",
    "    masks[mask_file] = mask_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try our mask on the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "input_path = f'../data/sample/1/{image_names[i]}'\n",
    "mask = masks[image_names[i][:-4] + \".png\"]\n",
    "plot_mask_on_image(input_path, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works well ! Let's define a function to modify our images, only keeping the roofs and making the rest of the image magenta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "def keep_roofs(\n",
    "    image_name: str, \n",
    "    input_path: str,\n",
    "    output_path: str,\n",
    "    mask: np.ndarray, \n",
    "    color: list[int] = [255, 0, 255]\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Overlay the mask on the image and keep only the pixels that are colored with the specified color.\n",
    "\n",
    "    Args:\n",
    "        image_name (str): The name of the image.\n",
    "        input_path (str): The path to the image.\n",
    "        output_path (str): The path to save the modified image.\n",
    "        mask (np.ndarray): The mask to overlay on the image.\n",
    "        color (list[int]): The color to keep, must be a 3-element list with the RGB values. Default is magenta, i.e. [255, 0, 255].\n",
    "    \n",
    "    Returns:\n",
    "        PIL.Image: The image with only the pixels colored with the specified color.\n",
    "    \"\"\"\n",
    "    image = Image.open(input_path + \"/\" + image_name)\n",
    "    image_array = np.array(image)\n",
    "    image_array[mask == 0] = color\n",
    "    modified_image = Image.fromarray(image_array)\n",
    "    modified_image.save(output_path + \"/\" + image_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "for name in tqdm(image_names):\n",
    "    keep_roofs(name, \"../data/sample/1\", \"../data/roofs/1\", masks[name[:-4] + \".png\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last sanity check, let's overlay the solar panels masks on top of the modified images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mask_on_image(f'../data/roofs/1/{image_names[0]}', labels[image_names[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_v2 = SolarPanelDataset(\"../data/roofs/1/\", image_names[:100], list(labels.values())[:100], transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_v2, val_dataset_v2 = torch.utils.data.random_split(dataset, [0.8, 0.2])\n",
    "train_loader_v2 = DataLoader(train_dataset_v2, batch_size=8, shuffle=True)\n",
    "val_loader_v2 = DataLoader(val_dataset_v2, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet().to(device)\n",
    "criterion = nn.BCELoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 4\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, masks in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{epochs}\", unit=\"batch\"):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"Validation Loss: {val_loss/len(val_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "sample_image, sample_mask = dataset_v2[15]\n",
    "sample_image = sample_image.unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    predicted_output = model(sample_image)\n",
    "\n",
    "\n",
    "input_image = sample_image.cpu().squeeze().permute(1, 2, 0)\n",
    "if isinstance(sample_mask, np.ndarray):\n",
    "    ground_truth_mask = torch.tensor(sample_mask)\n",
    "\n",
    "ground_truth_mask = ground_truth_mask.cpu().squeeze()\n",
    "predicted_mask = predicted_output.cpu().squeeze()\n",
    "\n",
    "\n",
    "binary_predicted_mask = np.where(predicted_mask > 0.01, 1, 0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(15, 5))\n",
    "# Input image\n",
    "axes[0].imshow(input_image)\n",
    "axes[0].set_title(\"Input Image\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Ground truth mask\n",
    "axes[1].imshow(ground_truth_mask, cmap=\"gray\")\n",
    "axes[1].set_title(\"Ground Truth Mask\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Predicted mask\n",
    "axes[2].imshow(predicted_mask, cmap=\"gray\")\n",
    "axes[2].set_title(\"Predicted Mask\")\n",
    "axes[2].axis('off')\n",
    "\n",
    "# Thresholded predicted mask\n",
    "axes[3].imshow(binary_predicted_mask, cmap=\"gray\")\n",
    "axes[3].set_title(\"Thresholded Predicted Mask\")\n",
    "axes[3].axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
